# ğŸ’ Diamond Price Prediction

*ModÃ¨les de rÃ©gression & Ensemble Learning*

## ğŸ“Œ Description du Projet

Ce projet a pour objectif de **prÃ©dire le prix des diamants** Ã  partir de diffÃ©rentes caractÃ©ristiques (carat, coupe, couleur, clartÃ©, dimensions, etc.).
Pour amÃ©liorer la performance, plusieurs modÃ¨les de machine learning sont implÃ©mentÃ©s :

* **RÃ©gression LinÃ©aire**
* **Arbre de DÃ©cision**
* **Random Forest**
* **Ensemble Learning (moyenne ou stacking)**

Le projet met lâ€™accent sur :
âœ”ï¸ La prÃ©paration des donnÃ©es
âœ”ï¸ La comparaison des performances
âœ”ï¸ Lâ€™amÃ©lioration de la prÃ©cision grÃ¢ce aux modÃ¨les dâ€™ensemble
âœ”ï¸ Lâ€™interprÃ©tation des rÃ©sultats


## ğŸ“‚ Dataset

Le dataset utilisÃ© contient plusieurs colonnes telles que :

* `carat` â€” poids du diamant
* `cut` â€” qualitÃ© de la coupe
* `color` â€” couleur
* `clarity` â€” puretÃ©
* `x, y, z` â€” dimensions
* `price` â€” variable cible

Dataset typique : **diamonds.csv** (Kaggle).


## ğŸ§¹ PrÃ©traitement

Les Ã©tapes de prÃ©paration incluent :

* Gestion des valeurs manquantes
* Encodage des variables catÃ©gorielles (`cut`, `color`, `clarity`) via **OneHotEncoder**
* Normalisation/standardisation des variables numÃ©riques
* SÃ©paration train/test


## ğŸ§  ModÃ¨les ImplÃ©mentÃ©s

### ğŸ”¹ 1. RÃ©gression LinÃ©aire

ModÃ¨le simple permettant dâ€™obtenir une baseline.

**Points forts :**

* InterprÃ©table
* Rapide

### ğŸ”¹ 2. Arbre de DÃ©cision

Capture les relations non linÃ©aires.

**HyperparamÃ¨tres typiques :**

* `max_depth`
* `min_samples_split`

### ğŸ”¹ 3. ForÃªt AlÃ©atoire (Random Forest)

ModÃ¨le robuste basÃ© sur lâ€™agrÃ©gation de plusieurs arbres.

**Avantages :**

* Excellent compromis biais/variance
* GÃ¨re bien les features hÃ©tÃ©rogÃ¨nes
* RÃ©duit fortement lâ€™overfitting

### ğŸ”¹ 4. Ensemble Learning

Combinaison de modÃ¨les pour augmenter la prÃ©cision.
Deux approches possibles :

* **Voting Regressor** (moyenne des prÃ©dictions)
* **Stacking Regressor** (modÃ¨le mÃ©ta-apprenant)


## ğŸ“Š Ã‰valuation des ModÃ¨les

Les mÃ©triques utilisÃ©es sont :

* **RMSE** (Root Mean Squared Error)
* **MAE** (Mean Absolute Error)
* **RÂ² Score**

Exemple de code pour comparer :

```python
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

def evaluate(y_true, y_pred):
    return {
        "RMSE": mean_squared_error(y_true, y_pred, squared=False),
        "MAE": mean_absolute_error(y_true, y_pred),
        "R2": r2_score(y_true, y_pred)
    }
```


## ğŸ“ˆ RÃ©sultats (exemple)

| ModÃ¨le              | RMSE â†“  | RÂ² â†‘     |
| ------------------- | ------- | -------- |
| RÃ©gression LinÃ©aire | 1100    | 0.88     |
| Arbre de DÃ©cision   | 900     | 0.93     |
| Random Forest       | **650** | **0.97** |
| Ensemble Learning   | **620** | **0.98** |

*(Les chiffres peuvent varier selon lâ€™entraÃ®nement.)*


## ğŸš€ Technologies UtilisÃ©es

* **Python 3.x**
* `pandas`
* `numpy`
* `scikit-learn`
* `matplotlib` / `seaborn`


## â–¶ï¸ ExÃ©cuter le Projet

### 1. Cloner le dÃ©pÃ´t

```bash
git clone https://github.com/username/diamond-regression.git
cd diamond-regression
```

### 2. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

### 3. Lancer lâ€™entraÃ®nement

```bash
python train.py
```

### 4. Visualiser les rÃ©sultats

```bash
python evaluate.py
```


## âœ¨ AmÃ©liorations Futures

* Ajout de **XGBoost / LightGBM / CatBoost**
* Technique avancÃ©e : **hyperparameter tuning (GridSearchCV, Optuna)**
* Deployment via **FastAPI** ou **Streamlit**
* InterprÃ©tabilitÃ© via **SHAP values**


## ğŸ‘¤ Auteur

**Alex Alkhatib**
Projet de Data Science â€” ModÃ©lisation prÃ©dictive en Machine Learning


## ğŸ“„ Licence
MIT License
Copyright (c) 2025 Alex Alkhatib
